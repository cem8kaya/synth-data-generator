# ğŸ“Š TELECOM SYNTHETIC DATA GENERATOR - DETAYLI TEKNÄ°K ANALÄ°Z

## ğŸ¯ **UYGULAMA AMACI**

Bu uygulama, **telekom operatÃ¶rleri iÃ§in gerÃ§ekÃ§i sentetik network KPI verisi Ã¼reten** profesyonel bir sistemdir. AsÄ±l amacÄ±:
- Machine learning modellerinin eÄŸitimi
- Kapasite planlamasÄ± simÃ¼lasyonu  
- Anomali tespit algoritmalarÄ±nÄ±n geliÅŸtirilmesi
- Hassas operasyonel verilerin paylaÅŸÄ±lmadan test ortamlarÄ± oluÅŸturulmasÄ±

---

## ğŸ—ï¸ **MÄ°MARÄ° TASARIM - 6 KATMANLI MODEL**

Uygulama, **modÃ¼ler katmanlÄ± mimari** kullanarak Ã§alÄ±ÅŸÄ±r:

```
LAYER 6: Validation & Quality Assurance
           â†“ (validates)
LAYER 5: Anomaly Modeling & Scenarios
           â†“ (injects anomalies)
LAYER 4: Procedure Simulation
           â†“ (simulates protocols)
LAYER 3: Temporal Engineering & Time Series
           â†“ (applies seasonality, ARIMA)
LAYER 2: Multivariate Dependency Modeling
           â†“ (applies correlations, dependencies)
LAYER 1: Statistical Foundation
           â†“ (generates base distributions)
Raw Random Numbers
```

---

## ğŸ”¬ **LAYER 1: Ä°STATÄ°STÄ°KSEL TEMEL - DAÄILIM ÃœRETME KATMANI**

Bu katman, tÃ¼m sistemin matematiksel temelini oluÅŸturur ve Ã¼zerindeki tÃ¼m katmanlarÄ±n gÃ¼venle kullanabileceÄŸi istatistiksel olarak geÃ§erli sayÄ±lar Ã¼retir. Her network metriÄŸinin kendine Ã¶zgÃ¼ bir davranÄ±ÅŸ karakteristiÄŸi vardÄ±r ve bu karakteristikler belirli olasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ±yla en iyi ÅŸekilde modellenebilir.

### **Neden FarklÄ± DaÄŸÄ±lÄ±mlar KullanÄ±lÄ±r?**

GerÃ§ek dÃ¼nyada farklÄ± fenomenler farklÄ± istatistiksel davranÄ±ÅŸlar gÃ¶sterir. Ã–rneÄŸin, bir santraldeki telefon Ã§aÄŸrÄ±larÄ±nÄ±n gelme zamanlamasÄ± ile bu Ã§aÄŸrÄ±larÄ±n sÃ¼releri tamamen farklÄ± doÄŸalara sahiptir. Ã‡aÄŸrÄ±lar rastgele ve baÄŸÄ±msÄ±z olarak gelirken (Poisson), Ã§aÄŸrÄ± sÃ¼releri genellikle birkaÃ§ dakika etrafÄ±nda yoÄŸunlaÅŸÄ±r ama bazen Ã§ok uzun olabilir (Log-Normal). Ä°ÅŸte bu nedenle sistemimiz altÄ± farklÄ± daÄŸÄ±lÄ±m tipi kullanÄ±r.

### **KullanÄ±lan DaÄŸÄ±lÄ±mlar ve DetaylÄ± AÃ§Ä±klamalarÄ±**

| DaÄŸÄ±lÄ±m | KullanÄ±m AlanÄ± | Parametreler | Matematiksel Ã–zellik |
|---------|---------------|--------------|---------------------|
| **Poisson** | Event arrival rates, Ã§aÄŸrÄ± denemeleri | Î» (mean) | Discrete, non-negative integers |
| **Gamma** | Latency, gecikme, iÅŸleme sÃ¼releri | shape (Î±), scale (Î¸), CV | Continuous, positive, right-skewed |
| **Log-Normal** | Throughput, bandwidth, data volume | Î¼, Ïƒ (log-scale) | Continuous, positive, heavy-tailed |
| **Beta** | Success rates, availability (0-1 arasÄ±) | Î±, Î², stability level | Bounded [0,1], flexible shape |
| **Exponential** | Inter-arrival times, failure times | rate (Î») | Memoryless, decreasing probability |
| **Normal** | Genel metrikler | mean (Î¼), std (Ïƒ) | Symmetric, unbounded |

#### **Poisson DaÄŸÄ±lÄ±mÄ± - Event Arrival Modeling**

Poisson daÄŸÄ±lÄ±mÄ±, belirli bir zaman aralÄ±ÄŸÄ±nda meydana gelen baÄŸÄ±msÄ±z olaylarÄ±n sayÄ±sÄ±nÄ± modellemek iÃ§in kullanÄ±lÄ±r. Telekom networklerinde, SIP INVITE mesajlarÄ±nÄ±n gelme sÄ±klÄ±ÄŸÄ±, PDU session kurulum istekleri veya handover olaylarÄ± Poisson sÃ¼reciyle mÃ¼kemmel ÅŸekilde modellenir. 

Lambda parametresi ortalama olay sayÄ±sÄ±nÄ± temsil eder. Ã–rneÄŸin lambda yÃ¼z ise, dakikada ortalama yÃ¼z Ã§aÄŸrÄ± beklenir, ancak bazÄ± dakikalarda doksan dokuz, bazÄ±larÄ±nda yÃ¼z bir olabilir. DaÄŸÄ±lÄ±mÄ±n gÃ¼zel yanÄ±, bu varyasyonu matematiksel olarak tutarlÄ± bir ÅŸekilde Ã¼retmesidir.

GerÃ§ek dÃ¼nya uygulamasÄ±nda lambda sabit deÄŸildir. Gece yarÄ±sÄ± lambda on olabilirken, Ã¶ÄŸlen saatlerinde beÅŸ yÃ¼ze Ã§Ä±kabilir. Bu yÃ¼zden sistemimiz non-homogeneous Poisson process kullanÄ±r, yani lambda zamanÄ±n bir fonksiyonu olarak tanÄ±mlanÄ±r. Bu fonksiyon gÃ¼nlÃ¼k dÃ¶ngÃ¼leri, yoÄŸun saatleri ve haftalÄ±k pattern'leri iÃ§erir.

```python
# Poisson generation with time-varying lambda
base_lambda = 100  # Base rate
hour_of_day = timestamp.hour
time_factor = 1 + 0.3 * sin(2*pi * hour_of_day / 24)  # Daily cycle
busy_hour_boost = 1.5 if 8 <= hour_of_day <= 10 else 1.0
adjusted_lambda = base_lambda * time_factor * busy_hour_boost
event_count = np.random.poisson(adjusted_lambda)
```

#### **Gamma DaÄŸÄ±lÄ±mÄ± - Latency ve Timing Modeling**

Gamma daÄŸÄ±lÄ±mÄ±, network latency ve iÅŸleme sÃ¼releri iÃ§in ideal bir modeldir. Latency deÄŸerleri hiÃ§bir zaman negatif olamaz, genellikle belli bir deÄŸer etrafÄ±nda yoÄŸunlaÅŸÄ±r ama bazen Ã§ok yÃ¼ksek deÄŸerler alabilir, bu da saÄŸa Ã§arpÄ±k bir daÄŸÄ±lÄ±m oluÅŸturur.

Shape parametresi daÄŸÄ±lÄ±mÄ±n ÅŸeklini, scale parametresi ise yayÄ±lÄ±mÄ±nÄ± kontrol eder. Ä°lginÃ§ olan nokta, bu iki parametreyi doÄŸrudan ayarlamak yerine, ortalama ve coefficient of variation Ã¼zerinden hesaplamamÄ±zdÄ±r. CV deÄŸeri standart sapmanÄ±n ortalamaya oranÄ±dÄ±r ve network'Ã¼n ne kadar kararlÄ± Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir.

DÃ¼ÅŸÃ¼k CV deÄŸerli bir network Ã§ok tutarlÄ±dÄ±r, latency deÄŸerleri dar bir aralÄ±kta kalÄ±r. YÃ¼ksek CV deÄŸerli network ise deÄŸiÅŸkendir, bazen hÄ±zlÄ± bazen yavaÅŸ Ã§alÄ±ÅŸÄ±r. Ã–rneÄŸin, yÃ¼ksek kaliteli fiber optik network CV deÄŸeri sÄ±fÄ±r nokta bir civarÄ±nda olabilirken, congestion problemi yaÅŸayan wireless network CV deÄŸeri sÄ±fÄ±r nokta beÅŸ veya daha yÃ¼ksek olabilir.

```python
# Gamma parameter calculation from mean and CV
mean_latency = 50  # milliseconds
cv = 0.3  # Coefficient of variation
shape = 1 / (cv ** 2)  # Inverse square relationship
scale = mean_latency / shape
latency_values = np.random.gamma(shape, scale, size=n_samples)
```

Shape parametresi on bir ise, daÄŸÄ±lÄ±m neredeyse bell curve gibi simetriktir. Shape parametresi iki ise, belirgin saÄŸa Ã§arpÄ±klÄ±k vardÄ±r. Shape parametresi bir ise, exponential daÄŸÄ±lÄ±ma yaklaÅŸÄ±r. Sistemimiz genellikle shape deÄŸerlerini beÅŸ ile yirmi arasÄ±nda tutar, bu da gerÃ§ekÃ§i latency profilleri Ã¼retir.

#### **Log-Normal DaÄŸÄ±lÄ±mÄ± - Throughput ve Bandwidth Modeling**

Log-Normal daÄŸÄ±lÄ±m, throughput ve bandwidth gibi metrikler iÃ§in kullanÄ±lÄ±r. Bu daÄŸÄ±lÄ±mÄ±n Ã¶zel Ã¶zelliÄŸi, deÄŸerlerin logaritmasÄ± normal daÄŸÄ±lÄ±m gÃ¶sterir, ancak deÄŸerlerin kendisi son derece saÄŸa Ã§arpÄ±k bir profil sergiler. Bu, gerÃ§ek network throughput davranÄ±ÅŸÄ±nÄ± mÃ¼kemmel yansÄ±tÄ±r.

Ã‡oÄŸu kullanÄ±cÄ± orta seviye throughput alÄ±r, bazÄ± ÅŸanslÄ± kullanÄ±cÄ±lar yÃ¼ksek throughput alÄ±r, ama birkaÃ§ kullanÄ±cÄ± Ã§ok yÃ¼ksek throughput deÄŸerlerine ulaÅŸabilir. Ã–rneÄŸin median throughput beÅŸ megabit olabilirken, doksan beÅŸinci percentile yirmi megabit, maksimum deÄŸer elli megabit olabilir. Bu tip heavy-tailed distribution, log-normal ile doÄŸal olarak modellenir.

Sistemimiz log-normal parametrelerini median ve doksan beÅŸinci percentile deÄŸerlerinden tÃ¼retir. Bu yaklaÅŸÄ±m, kullanÄ±cÄ± iÃ§in daha sezgiseldir Ã§Ã¼nkÃ¼ doÄŸrudan gÃ¶zlemlenebilir metriklerle Ã§alÄ±ÅŸÄ±r. Mu ve sigma parametreleri bu hedef deÄŸerlerden ters hesaplama ile bulunur.

```python
# Log-normal parameter calculation from median and p95
median_throughput = 10  # Mbps
p95_throughput = 30  # Mbps
# Solve equations: median = exp(mu), p95 = exp(mu + 1.645*sigma)
sigma = (log(p95_throughput) - log(median_throughput)) / 1.645
mu = log(median_throughput)
throughput_values = np.random.lognormal(mu, sigma, size=n_samples)
```

#### **Beta DaÄŸÄ±lÄ±mÄ± - Success Rate ve Availability Modeling**

Beta daÄŸÄ±lÄ±mÄ±, sÄ±fÄ±r ile bir arasÄ±nda sÄ±nÄ±rlÄ± olan metrikler iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸtÄ±r. Success rate, availability percentage, packet delivery ratio gibi metrikler doÄŸasÄ± gereÄŸi bu aralÄ±ktadÄ±r ve beta daÄŸÄ±lÄ±mÄ± bunlarÄ± modellemek iÃ§in matematiksel olarak mÃ¼kemmel bir araÃ§tÄ±r.

Beta daÄŸÄ±lÄ±mÄ±nÄ±n gÃ¼zelliÄŸi, iki parametresiyle Ã§ok farklÄ± ÅŸekiller alabilmesidir. Alpha ve beta parametreleri her ikisi de bÃ¼yÃ¼k olduÄŸunda, daÄŸÄ±lÄ±m dar ve peaked olur. Bu, Ã§ok kararlÄ± bir network'Ã¼ temsil eder, success rate sÃ¼rekli doksan beÅŸ ile doksan sekiz arasÄ±nda salÄ±nÄ±r. Her iki parametre de kÃ¼Ã§Ã¼k olduÄŸunda, daÄŸÄ±lÄ±m U-shape alÄ±r, yani deÄŸerler ya Ã§ok dÃ¼ÅŸÃ¼k ya Ã§ok yÃ¼ksek olur, bu da problemli bir network'Ã¼ gÃ¶sterir.

Sistemimiz alpha ve beta parametrelerini iki kavramdan tÃ¼retir: target rate ve stability level. Target rate, istenen ortalama success rate'dir. Stability level ise bu target'Ä±n etrafÄ±nda ne kadar dar kalÄ±ndÄ±ÄŸÄ±nÄ± belirler. High stability, Ã§ok az varyasyon demektir. Low stability, geniÅŸ bir deÄŸiÅŸkenlik aralÄ±ÄŸÄ± demektir.

```python
# Beta parameter calculation from target and stability
target_rate = 0.95  # 95% success rate
stability = "HIGH"  # HIGH, MEDIUM, or LOW
concentration_map = {"HIGH": 50, "MEDIUM": 10, "LOW": 2}
concentration = concentration_map[stability]
alpha = target_rate * concentration  # = 0.95 * 50 = 47.5
beta = (1 - target_rate) * concentration  # = 0.05 * 50 = 2.5
success_rate_values = np.random.beta(alpha, beta, size=n_samples)
```

High stability ile target doksan beÅŸ percent kombinasyonu, alpha kÄ±rk yedi nokta beÅŸ ve beta iki nokta beÅŸ verir. Bu daÄŸÄ±lÄ±m, success rate deÄŸerlerini doksan Ã¼Ã§ ile doksan yedi arasÄ±nda sÄ±kÄ± bir ÅŸekilde tutar. Medium stability ise alpha dokuz nokta beÅŸ ve beta sÄ±fÄ±r nokta beÅŸ verir, bu da doksan ile doksan sekiz arasÄ± daha geniÅŸ bir daÄŸÄ±lÄ±m saÄŸlar.

#### **Exponential DaÄŸÄ±lÄ±mÄ± - Inter-arrival Times**

Exponential daÄŸÄ±lÄ±m, olaylar arasÄ± sÃ¼releri modellemek iÃ§in kullanÄ±lÄ±r. EÄŸer olaylar Poisson sÃ¼reciyle geliyorsa, olaylar arasÄ± sÃ¼reler otomatik olarak exponential daÄŸÄ±lÄ±m gÃ¶sterir. Bu iki daÄŸÄ±lÄ±m matematiksel olarak birbirinin ikilidir.

Exponential daÄŸÄ±lÄ±mÄ±n Ã¶nemli bir Ã¶zelliÄŸi memoryless property'sidir. Yani bir sonraki olayÄ±n ne zaman geleceÄŸi, en son olayÄ±n ne zaman geldiÄŸinden baÄŸÄ±msÄ±zdÄ±r. Bu, gerÃ§ek network trafiÄŸi iÃ§in genellikle geÃ§erli bir varsayÄ±mdÄ±r Ã§Ã¼nkÃ¼ farklÄ± kullanÄ±cÄ±lar birbirinden baÄŸÄ±msÄ±z hareket eder.

Rate parametresi lambda, birim zamanda beklenen olay sayÄ±sÄ±dÄ±r. EÄŸer lambda yÃ¼z ise, dakikada ortalama yÃ¼z olay olur, bu da olaylar arasÄ± ortalama sÃ¼renin sÄ±fÄ±r nokta sÄ±fÄ±r bir dakika yani altÄ± yÃ¼z milisaniye olmasÄ± demektir. Exponential daÄŸÄ±lÄ±m, bu altÄ± yÃ¼z milisaniye etrafÄ±nda deÄŸerler Ã¼retir ama bÃ¼yÃ¼k varyasyonla, bazÄ± olaylar hemen arkasÄ±ndan gelir, bazÄ±larÄ± gecikmeli gelir.

---

## ğŸ”— **LAYER 2: Ã‡OK DEÄÄ°ÅKENLÄ° BAÄIMLILIKLAR - KORELASYON VE NEDENSELLIK KATMANI**

Bu katman, Layer 1'in Ã¼rettiÄŸi baÄŸÄ±msÄ±z deÄŸerleri alÄ±r ve aralarÄ±nda gerÃ§ek dÃ¼nyada gÃ¶zlemlenen iliÅŸkileri kurar. GerÃ§ek telekom networklerinde metrikler izole deÄŸildir, karmaÅŸÄ±k bir baÄŸÄ±mlÄ±lÄ±k aÄŸÄ± iÃ§inde birbirleriyle etkileÅŸir. Latency arttÄ±ÄŸÄ±nda success rate dÃ¼ÅŸer, throughput yÃ¼kseldiÄŸinde CPU kullanÄ±mÄ± artar, packet loss olduÄŸunda voice quality bozulur. Bu katman, bu iliÅŸkileri matematiksel olarak modelleyerek sentetik verinin gerÃ§ekÃ§iliÄŸini dramatik ÅŸekilde artÄ±rÄ±r.

### **Gaussian Copula - Korelasyon YapÄ±sÄ±nÄ± Koruma**

Gaussian Copula, farklÄ± daÄŸÄ±lÄ±mlara sahip deÄŸiÅŸkenler arasÄ±nda korelasyon yaratmanÄ±n zarif bir yÃ¶ntemidir. Temel fikir ÅŸudur: Ã¶nce deÄŸiÅŸkenleri uniform daÄŸÄ±lÄ±ma transform edersiniz, uniform deÄŸerleri correlated normal deÄŸiÅŸkenlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼rsÃ¼nÃ¼z, sonra bu normal deÄŸiÅŸkenleri istediÄŸiniz orijinal daÄŸÄ±lÄ±mlara geri transform edersiniz.

SÃ¼recin ilk adÄ±mÄ±nda, her deÄŸiÅŸkenin kendi daÄŸÄ±lÄ±mÄ±nÄ±n cumulative distribution function'Ä± kullanÄ±larak uniform deÄŸerlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r. Ã–rneÄŸin, gamma daÄŸÄ±lÄ±mlÄ± bir latency deÄŸeri elli milisaniye ise ve bu deÄŸer daÄŸÄ±lÄ±mÄ±n yetmiÅŸ beÅŸinci percentile'Ä±nda ise, bu deÄŸer sÄ±fÄ±r nokta yetmiÅŸ beÅŸ uniform deÄŸerine map edilir. Bu iÅŸlem probability integral transform olarak bilinir ve her daÄŸÄ±lÄ±mdan uniform'a geÃ§iÅŸi garanti eder.

Ä°kinci adÄ±mda, bu uniform deÄŸerler standard normal daÄŸÄ±lÄ±ma transform edilir. Bu, inverse normal CDF kullanÄ±larak yapÄ±lÄ±r. SÄ±fÄ±r nokta yetmiÅŸ beÅŸ uniform deÄŸeri, yaklaÅŸÄ±k sÄ±fÄ±r nokta altmÄ±ÅŸ yedi standard normal deÄŸerine karÅŸÄ±lÄ±k gelir. Bu noktada tÃ¼m deÄŸiÅŸkenlerimiz standard normal uzayÄ±nda bulunur ve burada korelasyon matrisini uygulayabiliriz.

```python
# Gaussian Copula implementation
def apply_gaussian_copula(values_dict, correlation_matrix):
    # Step 1: Transform to uniform using empirical CDF
    uniform_values = {}
    for name, values in values_dict.items():
        ranks = stats.rankdata(values)
        uniform_values[name] = ranks / (len(values) + 1)
    
    # Step 2: Transform to standard normal
    normal_values = {}
    for name, u_vals in uniform_values.items():
        normal_values[name] = stats.norm.ppf(u_vals)
    
    # Step 3: Apply correlation via Cholesky decomposition
    L = np.linalg.cholesky(correlation_matrix)
    normal_matrix = np.column_stack([normal_values[name] for name in sorted(normal_values.keys())])
    correlated_normal = normal_matrix @ L.T
    
    # Step 4: Transform back to uniform
    correlated_uniform = stats.norm.cdf(correlated_normal)
    
    # Step 5: Transform back to original distributions
    result = {}
    for i, name in enumerate(sorted(values_dict.keys())):
        original_distribution = get_distribution_for_metric(name)
        result[name] = original_distribution.ppf(correlated_uniform[:, i])
    
    return result
```

ÃœÃ§Ã¼ncÃ¼ adÄ±m kritiktir: Cholesky decomposition kullanarak korelasyon matrisini uygularÄ±z. Korelasyon matrisi mutlaka positive semi-definite olmalÄ±dÄ±r, yani tÃ¼m eigenvalue'larÄ± sÄ±fÄ±r veya pozitif olmalÄ±dÄ±r. EÄŸer kullanÄ±cÄ± tutarsÄ±z korelasyonlar tanÄ±mlarsa, Ã¶rneÄŸin A ile B arasÄ±nda artÄ± bir, B ile C arasÄ±nda artÄ± bir, ama A ile C arasÄ±nda eksi bir korelasyon, bu matematiksel olarak imkansÄ±zdÄ±r. Sistemimiz bu durumda matrisi nearest valid matrix'e project eder veya kullanÄ±cÄ±yÄ± uyarÄ±r.

DÃ¶rdÃ¼ncÃ¼ adÄ±mda, correlated normal deÄŸerler tekrar uniform deÄŸerlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r, bu sefer normal CDF kullanÄ±larak. Son adÄ±mda, her deÄŸiÅŸken kendi orijinal daÄŸÄ±lÄ±mÄ±na geri transform edilir. Ã–rneÄŸin latency gamma daÄŸÄ±lÄ±mÄ±ndan geliyordu, ÅŸimdi correlated uniform deÄŸer gamma'nÄ±n inverse CDF'i ile gamma deÄŸerine geri dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.

Bu sÃ¼recin sonunda, her deÄŸiÅŸken kendi orijinal daÄŸÄ±lÄ±mÄ±nÄ± korurken, aralarÄ±nda istenen korelasyon yapÄ±sÄ± oluÅŸmuÅŸ olur. Bu, Gaussian Copula'nÄ±n en bÃ¼yÃ¼k avantajÄ±dÄ±r: marginal daÄŸÄ±lÄ±mlarÄ± bozmadan korelasyon yaratÄ±r.

### **Dependency Modeling - Nedensel Ä°liÅŸkiler**

Korelasyonun Ã¶tesinde, bazÄ± metrikler arasÄ±nda doÄŸrudan nedensel baÄŸlar vardÄ±r. Ã–rneÄŸin, network congestion olduÄŸunda hem latency artar hem de success rate dÃ¼ÅŸer. Ama bu iki etki aynÄ± anda ve baÄŸÄ±mlÄ± olarak gerÃ§ekleÅŸir. Dependency modeling, bir metriÄŸin deÄŸerinin diÄŸer metriklerin deÄŸerlerine gÃ¶re dinamik olarak ayarlanmasÄ±nÄ± saÄŸlar.

Sistemimiz basit bir dependency mechanism kullanÄ±r. Her metrik, baÄŸÄ±mlÄ± olduÄŸu metriklerin bir listesini iÃ§erir. Generation sÄ±rasÄ±nda, Ã¶nce tÃ¼m base deÄŸerler Ã¼retilir, sonra dependency pass'inde her metrik baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kontrol eder. EÄŸer bir dependency metrik dÃ¼ÅŸÃ¼k deÄŸerler alÄ±yorsa, baÄŸÄ±mlÄ± metrik de scale down edilir.

```python
# Dependency application example
def apply_dependencies(metric_values, metric_config, all_values_df):
    result = metric_values.copy()
    
    for dependency_name in metric_config.dependencies:
        if dependency_name in all_values_df.columns:
            dep_values = all_values_df[dependency_name].values
            
            # Normalize dependency to [0, 1]
            dep_normalized = (dep_values - dep_values.min()) / (dep_values.max() - dep_values.min())
            
            # Scale target metric: full value when dep=1, 70% value when dep=0
            scale_factor = 0.7 + 0.3 * dep_normalized
            result *= scale_factor
    
    return result
```

Ã–rneÄŸin, call success rate metriÄŸi network latency'ye baÄŸÄ±mlÄ± olarak tanÄ±mlanmÄ±ÅŸsa, latency deÄŸerleri yÃ¼ksek olduÄŸunda success rate otomatik olarak dÃ¼ÅŸÃ¼rÃ¼lÃ¼r. Bu, korelasyondan daha gÃ¼Ã§lÃ¼ bir iliÅŸkidir Ã§Ã¼nkÃ¼ direction of causality'yi encode eder.

### **Bayesian Network - Nedensel Graf YapÄ±sÄ±**

Bayesian Network, deÄŸiÅŸkenler arasÄ± nedensel iliÅŸkileri directed acyclic graph olarak modelleyen gÃ¼Ã§lÃ¼ bir araÃ§tÄ±r. Her node bir metriÄŸi temsil eder, her edge bir nedensel baÄŸlantÄ±yÄ± gÃ¶sterir. Ã–rneÄŸin, network load node'undan latency node'una bir edge varsa, bu "network load, latency'nin bir nedenidir" demektir.

Her node, parent node'larÄ±nÄ±n deÄŸerlerine baÄŸlÄ± olarak conditional probability distribution iÃ§erir. Ã–rneÄŸin, latency node'u "eÄŸer network load low ise latency distribution gamma with mean=30, eÄŸer network load high ise latency distribution gamma with mean=80" ÅŸeklinde bir CPD iÃ§erebilir.

Bayesian Network'Ã¼n generation sÃ¼reci topological sort ile yapÄ±lÄ±r. Ã–nce parent'Ä± olmayan node'lar generate edilir, sonra parent'larÄ± generate edilmiÅŸ node'lar, bÃ¶ylece her node generate edilirken parent deÄŸerleri hazÄ±rdÄ±r ve condition'lar doÄŸru ÅŸekilde uygulanabilir.

```python
# Bayesian Network generation with topological ordering
def generate_with_bayesian_network(network_structure, n_samples):
    # Topological sort: nodes with no parents first
    sorted_nodes = topological_sort(network_structure)
    results = {}
    
    for node in sorted_nodes:
        parent_values = {p: results[p] for p in node.parents}
        
        # Generate values conditional on parent values
        node_values = np.zeros(n_samples)
        for i in range(n_samples):
            # Get conditional distribution based on parent values at this sample
            parent_state = {p: parent_values[p][i] for p in node.parents}
            conditional_dist = node.get_conditional_distribution(parent_state)
            node_values[i] = conditional_dist.sample()
        
        results[node.name] = node_values
    
    return results
```

Sistemimizde Bayesian Network tam implement edilmemiÅŸ, ancak dependency mechanism'i onun basit bir versiyonu olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. Gelecek versiyonlarda, full Bayesian Network inference ve learning capabilities eklenebilir.

---

## â° **LAYER 3: TEMPORAL MÃœHENDÄ°SLÄ°ÄÄ° - ZAMAN SERÄ°SÄ° KATMANI**

Bu katman, veriye zamansal tutarlÄ±lÄ±k kazandÄ±rÄ±r. GerÃ§ek network metrikleri zaman iÃ§inde rastgele deÄŸildir, belirli pattern'ler ve smooth transitions gÃ¶sterir. Sabahtan Ã¶ÄŸleye geÃ§erken trafik aÅŸamalÄ± olarak artar, haftalÄ±k dÃ¶ngÃ¼ler tekrar eder, ani sÄ±Ã§ramalar olmaz. Temporal engineering olmadan, Ã¼retilen data anlamsÄ±z olur: bir dakika yÃ¼ksek trafik, sonraki dakika Ã§ok dÃ¼ÅŸÃ¼k trafik, ardÄ±ndan tekrar yÃ¼ksek. Bu gerÃ§ek network davranÄ±ÅŸÄ±nÄ± yansÄ±tmaz ve machine learning modelleri bu tip data ile temporal pattern'leri Ã¶ÄŸrenemez.

### **Fourier Series - Seasonality Modelleme**

Fourier Series, periyodik pattern'leri matematiksel olarak temsil etmenin elegant bir yoludur. Fourier Teoremi der ki, herhangi bir periyodik fonksiyon, sinÃ¼s ve cosinÃ¼s fonksiyonlarÄ±nÄ±n toplamÄ± olarak ifade edilebilir. Her sinÃ¼s ve cosinÃ¼s terimi bir "harmonic" olarak adlandÄ±rÄ±lÄ±r.

GÃ¼nlÃ¼k seasonality iÃ§in fundamental period yirmi dÃ¶rt saattir. Ä°lk harmonic, yirmi dÃ¶rt saatlik tam bir sinÃ¼s dalgasÄ±dÄ±r ve genel gÃ¼nlÃ¼k ritmi verir. Gece yarÄ±sÄ± minimum, Ã¶ÄŸlen maksimum, akÅŸam tekrar yÃ¼ksek, gece tekrar minimum ÅŸeklinde smooth bir dÃ¶ngÃ¼. Ä°kinci harmonic, on iki saatlik bir dÃ¶ngÃ¼dÃ¼r ve gÃ¼nde iki kez tekrar eden pattern'leri yakalar. Ã–rneÄŸin Ã¶ÄŸle ve akÅŸam olmak Ã¼zere iki peak olabilir. ÃœÃ§Ã¼ncÃ¼ harmonic sekiz saatlik dÃ¶ngÃ¼dÃ¼r ve daha ince detaylarÄ± yakalar.

```python
# Fourier Series seasonality implementation
def apply_fourier_seasonality(base_values, timestamps, period_hours=24, harmonics=3, amplitude=0.3):
    n = len(base_values)
    t = np.arange(n)
    
    # Base frequency: one complete cycle per period
    base_freq = 2 * np.pi / (period_hours * (60 / granularity_minutes))
    
    # Sum multiple harmonics with decreasing amplitude
    seasonal_component = np.zeros(n)
    for h in range(1, harmonics + 1):
        # Each harmonic has frequency h * base_freq
        harmonic_amplitude = amplitude / h  # Amplitude decreases with harmonic order
        phase = np.random.uniform(0, 2*np.pi)  # Random phase shift
        seasonal_component += harmonic_amplitude * np.sin(base_freq * h * t + phase)
    
    # Multiply base values by (1 + seasonal_component)
    # This gives percentage change relative to base
    result = base_values * (1 + seasonal_component)
    
    return result
```

Harmonik sayÄ±sÄ± kritik bir parametredir. Ã‡ok fazla harmonik kullanmak overfitting'e yol aÃ§ar, model noise'u da Ã¶ÄŸrenmeye Ã§alÄ±ÅŸÄ±r. Ã‡ok az harmonik kullanmak underfitting'e yol aÃ§ar, Ã¶nemli pattern'ler kaÃ§Ä±rÄ±lÄ±r. Pratikte Ã¼Ã§ ile beÅŸ arasÄ± harmonik, telecom network seasonality'si iÃ§in yeterlidir. Basit daily pattern iÃ§in iki harmonik yeter, kompleks pattern'ler iÃ§in dÃ¶rt veya beÅŸ harmonik kullanÄ±labilir.

Amplitude parametresi, seasonality'nin ne kadar gÃ¼Ã§lÃ¼ olduÄŸunu kontrol eder. Amplitude sÄ±fÄ±r nokta Ã¼Ã§ ise, deÄŸerler base value'larÄ±nÄ±n yÃ¼zde otuz Ã¼Ã§Ã¼ne kadar yukarÄ± veya aÅŸaÄŸÄ± salÄ±nabilir. Residential network'ler yÃ¼ksek amplitude gÃ¶sterir Ã§Ã¼nkÃ¼ kullanÄ±cÄ±lar gÃ¼ndÃ¼z Ã§ok aktif, gece Ã§ok pasiftir. Enterprise network'ler dÃ¼ÅŸÃ¼k amplitude gÃ¶sterir Ã§Ã¼nkÃ¼ ofis saatleri boyunca trafik nispeten stabÄ±ldÄ±r.

### **ARIMA Modeli - Temporal Smoothing ve Autocorrelation**

ARIMA modeli, zaman serisinin smooth olmasÄ±nÄ± ve temporal coherence gÃ¶stermesini saÄŸlar. ARIMA Ã¼Ã§ component'ten oluÅŸur: AutoRegressive, Integrated ve Moving Average. Sistemimizde sadece AR ve MA component'lerini kullanÄ±yoruz, I component'i (differencing) kullanmÄ±yoruz Ã§Ã¼nkÃ¼ verilerimiz zaten stationary.

AutoRegressive component, bir deÄŸerin geÃ§miÅŸ deÄŸerlerine baÄŸlÄ± olduÄŸunu sÃ¶yler. AR order iki ise, current value son iki value'ya baÄŸlÄ±dÄ±r. AR coefficient'lar bu baÄŸÄ±mlÄ±lÄ±ÄŸÄ±n gÃ¼cÃ¼nÃ¼ kontrol eder. Tipik AR coefficient'lar sÄ±fÄ±r nokta altÄ± ve sÄ±fÄ±r nokta Ã¼Ã§ gibi deÄŸerlerdir, birinci lag daha gÃ¼Ã§lÃ¼ etki eder, ikinci lag daha zayÄ±f.

```python
# ARIMA generation with AR and MA components
def apply_arima_smoothing(base_values, ar_order=2, ma_order=1, 
                          ar_coef=[0.6, 0.3], ma_coef=[0.4], noise_std=0.05):
    n = len(base_values)
    result = np.zeros(n)
    
    # Initialize first few values
    result[:ar_order] = base_values[:ar_order]
    
    # Generate noise sequence
    noise = np.random.normal(0, noise_std, n)
    
    for t in range(ar_order, n):
        # AR component: weighted sum of past values
        ar_component = sum(ar_coef[i] * result[t - i - 1] 
                          for i in range(ar_order))
        
        # MA component: weighted sum of past errors
        ma_component = sum(ma_coef[i] * noise[t - i - 1] 
                          for i in range(min(ma_order, t)))
        
        # Combine: weighted average of base value, AR, MA, and noise
        result[t] = (0.3 * base_values[t] + 
                     0.5 * ar_component + 
                     0.2 * ma_component + 
                     noise[t])
    
    return result
```

Moving Average component, geÃ§miÅŸ error'lara baÄŸlÄ±dÄ±r. Error, predicted value ile actual value arasÄ±ndaki farktÄ±r. MA order bir ise, current value son bir error'a baÄŸlÄ±dÄ±r. MA component, systematic error'larÄ± cancel etmeye yardÄ±mcÄ± olur ve time series'i daha smooth yapar.

Weight'ler Ã§ok Ã¶nemlidir. Sistemimiz tipik olarak base value'ya yÃ¼zde otuz, AR component'e yÃ¼zde elli, MA component'e yÃ¼zde yirmi aÄŸÄ±rlÄ±k verir. Bu, temporal smoothing'i saÄŸlarken base distribution'Ä±n Ã¶zelliklerini de korur. EÄŸer AR'a Ã§ok fazla aÄŸÄ±rlÄ±k verirsek, time series Ã§ok smooth olur ama base distribution'dan uzaklaÅŸÄ±r. EÄŸer base value'ya Ã§ok fazla aÄŸÄ±rlÄ±k verirsek, temporal coherence kaybolur.

AR coefficient'lar eksi bir ile bir arasÄ±nda olmalÄ±dÄ±r, aksi halde time series diverge eder veya wild oscillation gÃ¶sterir. Coefficient'larÄ±n toplamÄ± birden kÃ¼Ã§Ã¼k olmalÄ±dÄ±r, bu stationarity'yi garanti eder. MA coefficient'lar daha esnek olabilir ama genellikle eksi bir ile bir arasÄ±nda tutulur.

### **Change Point Detection and Injection**

Network'lerde structural change'ler vardÄ±r: software upgrade, hardware replacement, capacity expansion, configuration change. Bu change'ler KPI'larda ani veya aÅŸamalÄ± deÄŸiÅŸikliklere yol aÃ§ar. Change Point Injection modÃ¼lÃ¼, bu tip event'leri simÃ¼le eder.

ÃœÃ§ ana change type vardÄ±r. Step change, immediate ve permanent bir deÄŸiÅŸikliktir. Ã–rneÄŸin bir software patch anÄ±nda uygulanÄ±r ve performance'Ä± immediately improve eder. Step change'de deÄŸer bir seviyeden baÅŸka bir seviyeye instant geÃ§er. Implementation basittir: change timestamp'inden sonraki tÃ¼m deÄŸerler magnitude factor ile multiply edilir.

```python
# Step change application
def apply_step_change(values, change_time_idx, magnitude):
    result = values.copy()
    # magnitude = 0.2 means +20% increase
    result[change_time_idx:] *= (1 + magnitude)
    return result
```

Ramp change, gradual bir deÄŸiÅŸikliktir. Ã–rneÄŸin capacity expansion birkaÃ§ gÃ¼n boyunca aÅŸamalÄ± olarak deploy edilir. Ramp change'de deÄŸer starting level'dan ending level'a linear veya smooth curve ile geÃ§er. Duration parametresi transition'Ä±n ne kadar sÃ¼rdÃ¼ÄŸÃ¼nÃ¼ belirtir. Implementation lineer interpolation kullanÄ±r: her time step'te magnitude bir miktar artar, sonunda tam magnitude'a ulaÅŸÄ±r.

```python
# Ramp change application
def apply_ramp_change(values, start_idx, duration_steps, magnitude):
    result = values.copy()
    end_idx = min(start_idx + duration_steps, len(result))
    
    # Linear ramp from 0 to magnitude
    ramp = np.linspace(0, magnitude, end_idx - start_idx)
    result[start_idx:end_idx] *= (1 + ramp)
    
    # Full magnitude after ramp completes
    result[end_idx:] *= (1 + magnitude)
    
    return result
```

Spike change, temporary bir deÄŸiÅŸikliktir. Ã–rneÄŸin bir special event, bÃ¼yÃ¼k spor maÃ§Ä± veya konser, geÃ§ici bir trafik artÄ±ÅŸÄ±na yol aÃ§ar. Spike change'de deÄŸer hÄ±zla artar, bir peak'te tutar ve sonra hÄ±zla geri dÃ¼ÅŸer. Shape Gaussian bell curve ile modellenir. Peak'e yaklaÅŸtÄ±kÃ§a magnitude artar, peak'ten uzaklaÅŸtÄ±kÃ§a azalÄ±r.

Spike implementation Gaussian function kullanÄ±r. Center point peak zamanÄ±dÄ±r. Width parametresi spike'Ä±n ne kadar geniÅŸ olduÄŸunu kontrol eder. Her time step'te Gaussian function evaluate edilir ve magnitude buna gÃ¶re uygulanÄ±r.

---

## ğŸ”¬ **LAYER 4: PROSEDÃœR SÄ°MÃœLASYONU - PROTOKOL MODELLEME KATMANI**

Bu katman gerÃ§ek network protokollerinin davranÄ±ÅŸÄ±nÄ± taklit eder. SIP Ã§aÄŸrÄ± akÄ±ÅŸlarÄ±, NAS kayÄ±t prosedÃ¼rleri, GTP session kurulumlarÄ± gibi telecom spesifik iÅŸlemleri adÄ±m adÄ±m simÃ¼le eder. Her prosedÃ¼r kendi state machine'ini takip eder ve network koÅŸullarÄ±na gÃ¶re baÅŸarÄ± veya baÅŸarÄ±sÄ±zlÄ±k Ã¼retir.

### **State Machine Modeling - Durum GeÃ§iÅŸleri**

Her prosedÃ¼r bir state machine olarak tasarlanÄ±r. State machine, olasÄ± durumlarÄ± ve durumlar arasÄ± geÃ§iÅŸleri tanÄ±mlar. Ã–rneÄŸin bir SIP call flow ÅŸu state'leri iÃ§erir: IDLE, INVITE_SENT, TRYING, RINGING, PROGRESS, ESTABLISHED, BYE_SENT, TERMINATED.

Her transition bir olasÄ±lÄ±ÄŸa sahiptir. INVITE_SENT'den TRYING'e geÃ§iÅŸ olasÄ±lÄ±ÄŸÄ± Ã§ok yÃ¼ksektir, neredeyse yÃ¼zde doksan dokuz nokta dokuz, Ã§Ã¼nkÃ¼ bu neredeyse her zaman gerÃ§ekleÅŸir. Ancak RINGING'den ESTABLISHED'e geÃ§iÅŸ olasÄ±lÄ±ÄŸÄ± daha dÃ¼ÅŸÃ¼ktÃ¼r, belki yÃ¼zde doksan beÅŸ, Ã§Ã¼nkÃ¼ kullanÄ±cÄ± aramayÄ± reddetebilir veya timeout olabilir.

```python
# State machine implementation for SIP call
class SIPCallStateMachine:
    def __init__(self, network_condition):
        self.state = "IDLE"
        self.network_condition = network_condition
        self.timestamps = {}
        
    def execute_call(self):
        self.state = "INVITE_SENT"
        self.timestamps['invite'] = current_time()
        
        # Transition probabilities adjusted by network condition
        if self.network_condition == "GOOD":
            trying_prob = 0.999
            ringing_prob = 0.98
            established_prob = 0.95
        elif self.network_condition == "DEGRADED":
            trying_prob = 0.95
            ringing_prob = 0.90
            established_prob = 0.85
        else:  # POOR
            trying_prob = 0.85
            ringing_prob = 0.75
            established_prob = 0.70
        
        # Execute state transitions
        if random.random() < trying_prob:
            self.state = "TRYING"
            self.timestamps['trying'] = current_time()
        else:
            self.state = "FAILED"
            return self.get_metrics()
        
        if random.random() < ringing_prob:
            self.state = "RINGING"
            self.timestamps['ringing'] = current_time()
        else:
            self.state = "FAILED"
            return self.get_metrics()
        
        if random.random() < established_prob:
            self.state = "ESTABLISHED"
            self.timestamps['established'] = current_time()
            # Simulate call duration
            time.sleep(random.expovariate(1/120))  # Average 120 seconds
            self.state = "BYE_SENT"
            self.state = "TERMINATED"
        else:
            self.state = "FAILED"
        
        return self.get_metrics()
    
    def get_metrics(self):
        setup_latency = self.timestamps.get('established', 0) - self.timestamps['invite']
        success = (self.state == "TERMINATED")
        return {
            'setup_latency': setup_latency,
            'call_success': success,
            'failure_stage': self.state if not success else None
        }
```

Context-aware probability adjustment Ã§ok Ã¶nemlidir. Transition olasÄ±lÄ±klarÄ± statik deÄŸildir, network context'ine gÃ¶re dinamik olarak ayarlanÄ±r. Network congestion varsa, her transition'da failure olasÄ±lÄ±ÄŸÄ± artar. High latency varsa, timeout olasÄ±lÄ±ÄŸÄ± artar. Bu yaklaÅŸÄ±m, prosedÃ¼rlerin gerÃ§ekÃ§i bir ÅŸekilde fail etmesini saÄŸlar.

### **Counter ve KPI Hesaplama**

Her prosedÃ¼r execution counter'lara yansÄ±tÄ±lÄ±r. Attempt counter, baÅŸlatÄ±lan prosedÃ¼r sayÄ±sÄ±nÄ± sayar. Success counter, baÅŸarÄ±lÄ± tamamlanan prosedÃ¼r sayÄ±sÄ±nÄ± sayar. Failure counter, her failure reason iÃ§in ayrÄ± ayrÄ± tutulur: timeout failure, reject failure, error failure.

Bu counter'lar periyodik olarak aggregate edilir ve KPI'lara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r. Success rate, success counter'Ä±n attempt counter'a oranÄ±dÄ±r. Average setup latency, tÃ¼m setup latency deÄŸerlerinin ortalamasÄ±dÄ±r. Percentile latency'ler, sorted latency array'inden hesaplanÄ±r.

```python
# Counter aggregation and KPI calculation
class MetricAggregator:
    def __init__(self, window_duration_minutes=15):
        self.window_duration = window_duration_minutes
        self.counters = defaultdict(int)
        self.latency_samples = []
        
    def record_procedure(self, metrics):
        self.counters['attempt'] += 1
        if metrics['call_success']:
            self.counters['success'] += 1
            self.latency_samples.append(metrics['setup_latency'])
        else:
            self.counters[f"failure_{metrics['failure_stage']}"] += 1
    
    def calculate_kpis(self):
        kpis = {}
        
        # Success rate
        if self.counters['attempt'] > 0:
            kpis['success_rate'] = self.counters['success'] / self.counters['attempt']
        else:
            kpis['success_rate'] = 0.0
        
        # Latency statistics
        if self.latency_samples:
            kpis['avg_latency'] = np.mean(self.latency_samples)
            kpis['p50_latency'] = np.percentile(self.latency_samples, 50)
            kpis['p95_latency'] = np.percentile(self.latency_samples, 95)
            kpis['p99_latency'] = np.percentile(self.latency_samples, 99)
        else:
            kpis['avg_latency'] = 0.0
            kpis['p50_latency'] = 0.0
            kpis['p95_latency'] = 0.0
            kpis['p99_latency'] = 0.0
        
        # Failure breakdown
        total_failures = self.counters['attempt'] - self.counters['success']
        if total_failures > 0:
            for key, value in self.counters.items():
                if key.startswith('failure_'):
                    failure_type = key.replace('failure_', '')
                    kpis[f'{failure_type}_rate'] = value / self.counters['attempt']
        
        return kpis
```

---

## ğŸš¨ **LAYER 5: ANOMALÄ° MODELLEME - PROBLEM SÄ°MÃœLASYONU KATMANI**

GerÃ§ek telecom network'lerinde her ÅŸey her zaman mÃ¼kemmel Ã§alÄ±ÅŸmaz. Hardware failure'lar, software bug'lar, capacity bottleneck'ler, external attack'ler ve konfigÃ¼rasyon hatalarÄ± anomalilere yol aÃ§ar. Anomali Modelleme modÃ¼lÃ¼ bu exceptional durumlarÄ± sistematik olarak simÃ¼le eder. Bu machine learning anomaly detection modellerinin eÄŸitimi iÃ§in kritiktir.

### **Anomali Tipleri ve Enjeksiyon MekanizmalarÄ±**

| Anomali Tipi | Etki MekanizmasÄ± | Network Nedeni | Propagation DavranÄ±ÅŸÄ± |
|--------------|------------------|----------------|----------------------|
| **SPIKE** | Ani artÄ±ÅŸ: value *= (1 + severity) | Traffic burst, DDoS attack | Dependency graph Ã¼zerinden yayÄ±lÄ±r |
| **DROP** | Ani dÃ¼ÅŸÃ¼ÅŸ: value *= (1 - severity) | Service interruption, failover | Ä°lgili metriklere cascade eder |
| **OSCILLATION** | SalÄ±nÄ±m: sin-wave pattern | Misconfiguration, feedback loop | Periodic pattern oluÅŸturur |
| **CONGESTION** | Karma etki: successâ†“, latencyâ†‘ | Capacity exceeded, overload | TÃ¼m performance metriklerini etkiler |
| **DEGRADATION** | Kademeli bozulma | Hardware aging, memory leak | Zamanla ÅŸiddetlenir |
| **OUTAGE** | Tam kesinti: value *= 0.2 | Complete failure, power loss | Ä°lgili tÃ¼m servisleri etkiler |

### **Spike Anomaly - Traffic Burst**

Spike anomaly, kÄ±sa sÃ¼reli ani artÄ±ÅŸlarÄ± simÃ¼le eder. Bu bir flash crowd event, viral content, veya DDoS attack'den kaynaklanabilir. Spike'Ä±n karakteristiÄŸi, hÄ±zlÄ± yÃ¼kseliÅŸ, peak'te durma ve hÄ±zlÄ± dÃ¼ÅŸÃ¼ÅŸtÃ¼r. Duration tipik olarak on beÅŸ dakika ile iki saat arasÄ±ndadÄ±r.

Implementation straightforward'dÄ±r: anomaly'nin aktif olduÄŸu zaman pencerelerinde, etkilenen metriklerin deÄŸerleri severity factor ile multiply edilir. Severity sÄ±fÄ±r nokta beÅŸ ise, deÄŸerler yÃ¼zde elli artar. Severity bir ise deÄŸerler iki katÄ±na Ã§Ä±kar.

```python
# Spike anomaly injection
def inject_spike_anomaly(values, start_idx, duration_steps, severity):
    result = values.copy()
    end_idx = min(start_idx + duration_steps, len(result))
    
    # Simple approach: constant multiplier during spike period
    result[start_idx:end_idx] *= (1 + severity)
    
    return result
```

Daha sophisticated bir yaklaÅŸÄ±m, spike'a Gaussian shape vermektir. Bu durumda magnitude peak'te maksimum, baÅŸlangÄ±Ã§ ve bitiÅŸte sÄ±fÄ±ra yakÄ±ndÄ±r, smooth bir geÃ§iÅŸ saÄŸlar.

### **Congestion Anomaly - Network Overload**

Congestion anomaly en kompleks tiplerden biridir Ã§Ã¼nkÃ¼ farklÄ± metrikleri farklÄ± ÅŸekilde etkiler. Network congestion olduÄŸunda, latency artar Ã§Ã¼nkÃ¼ paketler queue'larda bekler. Success rate dÃ¼ÅŸer Ã§Ã¼nkÃ¼ timeout'lar ve drop'lar artar. Throughput dÃ¼ÅŸer Ã§Ã¼nkÃ¼ TCP congestion control throttle yapar. Packet loss artar Ã§Ã¼nkÃ¼ buffer'lar overflow olur.

Sistemimiz congestion'Ä± metric type'a gÃ¶re farklÄ± uygular. EÄŸer metric name'de "latency" veya "time" varsa, deÄŸerler artÄ±rÄ±lÄ±r. EÄŸer "success" veya "rate" varsa, deÄŸerler azaltÄ±lÄ±r. EÄŸer "loss" veya "error" varsa, deÄŸerler artÄ±rÄ±lÄ±r.

```python
# Congestion anomaly with metric-specific effects
def inject_congestion_anomaly(values, metric_name, start_idx, duration_steps, severity):
    result = values.copy()
    end_idx = min(start_idx + duration_steps, len(result))
    
    # Different effects based on metric type
    if any(keyword in metric_name.lower() for keyword in ['latency', 'time', 'delay']):
        # Latency increases during congestion
        result[start_idx:end_idx] *= (1 + severity * 2.0)  # Can double or triple
    
    elif any(keyword in metric_name.lower() for keyword in ['success', 'rate', 'availability']):
        # Success rate decreases during congestion
        result[start_idx:end_idx] *= (1 - severity * 0.5)  # Can drop by 50%
    
    elif any(keyword in metric_name.lower() for keyword in ['throughput', 'bandwidth']):
        # Throughput decreases during congestion
        result[start_idx:end_idx] *= (1 - severity * 0.6)  # Significant drop
    
    elif any(keyword in metric_name.lower() for keyword in ['loss', 'error', 'drop']):
        # Packet loss increases during congestion
        result[start_idx:end_idx] *= (1 + severity * 5.0)  # Can increase 5x
    
    return result
```

### **Anomaly Propagation - Cascade Effects**

GerÃ§ek network'lerde anomaliler izole kalmaz, dependency graph Ã¼zerinden yayÄ±lÄ±r. EÄŸer congestion downstream etkiler yaratÄ±r. EÄŸer bir critical service fail ederse, ona baÄŸlÄ± tÃ¼m servisler etkilenir. Propagation flag true ise, anomaly sadece epicenter metriÄŸi etkilemez, dependency graph Ã¼zerinden reachable tÃ¼m metrikleri de etkiler.

Propagation implementation graph traversal algoritmasÄ± kullanÄ±r. Epicenter'dan baÅŸlayarak breadth-first search yapar, her reachable node'u bulur. Her hop'ta severity bir miktar azalÄ±r, yani epicenter'dan uzaklaÅŸtÄ±kÃ§a etki zayÄ±flar. Bu realistic Ã§Ã¼nkÃ¼ gerÃ§ek dÃ¼nyada da anomaly effects distance ile decay eder.

---

## âœ… **LAYER 6: VALIDASYON VE KALÄ°TE GÃœVENCESÄ° KATMANI**

Bu katman Ã¼retilen verinin tÃ¼m kalite kriterlerini karÅŸÄ±layÄ±p karÅŸÄ±lamadÄ±ÄŸÄ±nÄ± kontrol eder. Validation multi-dimensional'dÄ±r: istatistiksel, mantÄ±ksal, nedensel, temporal ve domain-specific testler iÃ§erir. Her test dimension'Ä± bir score Ã¼retir ve weighted combination ile overall quality score hesaplanÄ±r.

### **Statistical Validation - DaÄŸÄ±lÄ±m UygunluÄŸu**

Ä°statistiksel validation, generated deÄŸerlerin intended distribution'a uyup uymadÄ±ÄŸÄ±nÄ± kontrol eder. Ä°lk check, NaN ve Inf deÄŸerlerinin olmamasÄ±dÄ±r. EÄŸer herhangi bir NaN veya Inf varsa, bu ciddi bir implementation bug'Ä± iÅŸaret eder.

Ä°kinci check, empirical moment'larÄ±n theoretical moment'lara yakÄ±nlÄ±ÄŸÄ±dÄ±r. Mean, variance, skewness ve kurtosis hesaplanÄ±r ve hedef deÄŸerlerle karÅŸÄ±laÅŸtÄ±rÄ±lÄ±r. Ã–rneÄŸin gamma distribution iÃ§in skewness iki bÃ¶lÃ¼ square root of shape olmalÄ±dÄ±r, empirical skewness bu formÃ¼le yakÄ±n olmalÄ±dÄ±r.

```python
# Statistical validation implementation
def validate_distribution_fit(values, expected_distribution):
    scores = {}
    
    # Check for invalid values
    has_nan = np.isnan(values).any()
    has_inf = np.isinf(values).any()
    scores['validity'] = 1.0 if not (has_nan or has_inf) else 0.0
    
    # Moment matching
    empirical_mean = np.mean(values)
    empirical_std = np.std(values)
    expected_mean = expected_distribution.mean
    expected_std = expected_distribution.std
    
    mean_error = abs(empirical_mean - expected_mean) / expected_mean
    std_error = abs(empirical_std - expected_std) / expected_std
    
    scores['mean_match'] = max(0, 1 - mean_error)
    scores['std_match'] = max(0, 1 - std_error)
    
    # Kolmogorov-Smirnov test for distribution fit
    ks_statistic, ks_pvalue = stats.kstest(values, expected_distribution.cdf)
    scores['ks_test'] = 1.0 if ks_pvalue > 0.05 else ks_pvalue / 0.05
    
    # Overall statistical score
    overall = np.mean([scores['validity'], scores['mean_match'], 
                       scores['std_match'], scores['ks_test']])
    
    return overall, scores
```

Kolmogorov-Smirnov test kritik bir testtir. Bu test, empirical CDF ile theoretical CDF arasÄ±ndaki maximum distance'Ä± Ã¶lÃ§er. EÄŸer distance kÃ¼Ã§Ã¼kse ve p-value yÃ¼ksekse, distribution fit iyidir. P-value sÄ±fÄ±r nokta sÄ±fÄ±r beÅŸten bÃ¼yÃ¼kse, yÃ¼zde doksan beÅŸ confidence ile distribution'Ä±n doÄŸru olduÄŸunu sÃ¶yleyebiliriz.

### **Logical Validation - Constraint Checking**

MantÄ±ksal validation, domain-specific constraint'lerin korunduÄŸunu kontrol eder. QoS boundaries birinci constraint'tir. Her metrik iÃ§in minimum ve maksimum deÄŸerler tanÄ±mlanmÄ±ÅŸsa, tÃ¼m generated deÄŸerler bu boundaries iÃ§inde olmalÄ±dÄ±r. Tek bir violation bile logical validation'Ä± fail eder.

```python
# Logical validation with QoS boundaries
def validate_logical_constraints(df, metric_configs):
    scores = {}
    
    for config in metric_configs:
        col_name = config.full_name
        violations = 0
        total_samples = len(df)
        
        if config.qos_min is not None:
            violations += (df[col_name] < config.qos_min).sum()
        
        if config.qos_max is not None:
            violations += (df[col_name] > config.qos_max).sum()
        
        # Score based on violation percentage
        violation_rate = violations / total_samples
        scores[col_name] = max(0, 1 - violation_rate * 10)  # Penalize heavily
    
    overall = np.mean(list(scores.values()))
    return overall, scores
```

Ordering constraint'ler baÅŸka bir logical check'tir. Ã–rneÄŸin p50 latency her zaman p95 latency'den kÃ¼Ã§Ã¼k olmalÄ±dÄ±r. Minimum throughput her zaman maximum throughput'tan kÃ¼Ã§Ã¼k olmalÄ±dÄ±r. Bu ordering relations violated ise, data internally inconsistent'tir.

### **Temporal Validation - Zaman Serisi Ã–zellikleri**

Temporal validation, time series'in expected temporal properties'i gÃ¶sterip gÃ¶stermediÄŸini kontrol eder. Autocorrelation structure ilk check'tir. SaÄŸlÄ±klÄ± bir time series, pozitif autocorrelation gÃ¶sterir: bugÃ¼nkÃ¼ deÄŸer dÃ¼nkÃ¼ deÄŸere benzer. Lag bir autocorrelation tipik olarak sÄ±fÄ±r nokta altÄ± ile sÄ±fÄ±r nokta sekiz arasÄ±ndadÄ±r.

```python
# Temporal validation with autocorrelation
def validate_temporal_properties(time_series):
    scores = {}
    
    # Autocorrelation at lag 1
    acf_lag1 = pd.Series(time_series).autocorr(lag=1)
    # Should be positive and significant
    scores['autocorr'] = 1.0 if 0.3 < acf_lag1 < 0.9 else max(0, acf_lag1)
    
    # Stationarity test (Augmented Dickey-Fuller)
    from statsmodels.tsa.stattools import adfuller
    adf_result = adfuller(time_series)
    adf_pvalue = adf_result[1]
    # p-value < 0.05 means stationary (good)
    scores['stationarity'] = 1.0 if adf_pvalue < 0.05 else 1 - adf_pvalue
    
    # Smoothness check: ratio of consecutive differences
    diffs = np.diff(time_series)
    smoothness = 1 - (np.std(diffs) / np.std(time_series))
    scores['smoothness'] = max(0, smoothness)
    
    overall = np.mean(list(scores.values()))
    return overall, scores
```

Seasonality detection baÅŸka bir temporal check'tir. EÄŸer seasonality enabled ise, generated time series'te periyodik pattern gÃ¶rÃ¼lmelidir. FFT kullanarak dominant frequency'leri buluruz ve expected seasonality period'una karÅŸÄ±lÄ±k geldiklerini doÄŸrularÄ±z.

### **Overall Quality Score Calculation**

Final quality score, tÃ¼m dimension score'larÄ±nÄ±n weighted average'Ä±dÄ±r. Weight'ler configuration'da tanÄ±mlanÄ±r ve sistemin hangi aspect'lere daha Ã§ok Ã¶nem verdiÄŸini gÃ¶sterir. Tipik weight distribution: statistical yÃ¼zde yirmi beÅŸ, logical yÃ¼zde yirmi beÅŸ, causal yÃ¼zde yirmi, temporal yÃ¼zde on beÅŸ, domain yÃ¼zde on beÅŸ.

```python
# Overall quality score calculation
def calculate_overall_quality(validation_results, weights):
    overall_score = (
        validation_results['statistical'] * weights.statistical_weight +
        validation_results['logical'] * weights.logical_weight +
        validation_results['causal'] * weights.causal_weight +
        validation_results['temporal'] * weights.temporal_weight +
        validation_results['domain'] * weights.domain_weight
    )
    
    quality_grade = 'EXCELLENT' if overall_score > 0.95 else \
                    'GOOD' if overall_score > 0.85 else \
                    'ACCEPTABLE' if overall_score > 0.75 else \
                    'POOR'
    
    return overall_score, quality_grade
```

EÄŸer quality score threshold'un altÄ±nda ise, sistem uyarÄ± verir. KullanÄ±cÄ± generation parameters'Ä± adjust edebilir ve yeniden Ã¼retim yapabilir. Bu iterative quality improvement sÃ¼reci, yÃ¼ksek kaliteli sentetik data garantiler.

---

## ğŸ”„ **Ã‡ALIÅMA AKIÅI (TEXT-BASED FLOW)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. BAÅLANGIÃ‡ - CONFIGURATION PARSING                        â”‚
â”‚    User Input (JSON/YAML) â†’ parse_config()                  â”‚
â”‚    â””â”€> GeneratorConfig object oluÅŸturulur                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. TIME WINDOW GENERATION                                   â”‚
â”‚    pd.date_range(start_time, end_time, freq=granularity)    â”‚
â”‚    â””â”€> Zaman damgalarÄ± listesi oluÅŸturulur                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LAYER 1 - BASE DISTRIBUTION GENERATION                   â”‚
â”‚    For each node and metric:                                â”‚
â”‚      â”œâ”€> Select distribution type (Poisson, Gamma, etc.)    â”‚
â”‚      â”œâ”€> Calculate parameters (shape, scale, Î±, Î²)          â”‚
â”‚      â””â”€> Generate random values: np.random.{dist}()         â”‚
â”‚    Result: raw_values[n_windows]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. LAYER 3 - TEMPORAL PATTERNS APPLICATION                  â”‚
â”‚    A) Seasonality (Fourier Series):                         â”‚
â”‚       â””â”€> Apply daily/weekly cycles                         â”‚
â”‚           values *= (1 + Î£ sin(harmonics))                  â”‚
â”‚                                                              â”‚
â”‚    B) ARIMA Smoothing:                                      â”‚
â”‚       â””â”€> Apply AR and MA components                        â”‚
â”‚           value[t] = f(value[t-1..t-p], noise[t-1..t-q])   â”‚
â”‚                                                              â”‚
â”‚    Result: temporal_values[n_windows]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. CHANGE POINTS APPLICATION                                â”‚
â”‚    For each change_point in config:                         â”‚
â”‚      â”œâ”€> Find timestamp index in time_windows               â”‚
â”‚      â”œâ”€> Apply change based on type:                        â”‚
â”‚      â”‚    â€¢ STEP: instant *= (1 + magnitude)               â”‚
â”‚      â”‚    â€¢ RAMP: gradual over duration                     â”‚
â”‚      â”‚    â€¢ SPIKE: Gaussian bump                            â”‚
â”‚      â””â”€> Update values[idx:end]                             â”‚
â”‚    Result: changed_values[n_windows]                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. LAYER 5 - ANOMALY INJECTION                             â”‚
â”‚    For each anomaly in config:                              â”‚
â”‚      â”œâ”€> Find start/end indices                             â”‚
â”‚      â”œâ”€> Apply anomaly effect based on type:                â”‚
â”‚      â”‚    â€¢ SPIKE: values *= (1 + severity)                 â”‚
â”‚      â”‚    â€¢ CONGESTION: if 'latency' *= 2, 'success' /= 2  â”‚
â”‚      â”‚    â€¢ OUTAGE: values *= (1 - 0.8*severity)           â”‚
â”‚      â””â”€> Propagate if enabled (via dependency graph)        â”‚
â”‚    Result: anomalous_values[n_windows]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. LAYER 2 - DEPENDENCIES & CORRELATIONS                   â”‚
â”‚    A) Apply Dependencies:                                    â”‚
â”‚       â””â”€> If metric depends on others:                      â”‚
â”‚           value *= scale_factor(dependency_values)           â”‚
â”‚                                                              â”‚
â”‚    B) Apply Correlations (Gaussian Copula):                 â”‚
â”‚       â””â”€> Normalize â†’ Apply correlation matrix              â”‚
â”‚           â†’ Transform back                                   â”‚
â”‚                                                              â”‚
â”‚    Result: correlated_values[n_windows]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. QoS ENFORCEMENT                                          â”‚
â”‚    For each metric with qos_min/max:                        â”‚
â”‚      â”œâ”€> np.maximum(values, qos_min)                        â”‚
â”‚      â””â”€> np.minimum(values, qos_max)                        â”‚
â”‚    Result: bounded_values[n_windows]                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. DATAFRAME CONSTRUCTION                                   â”‚
â”‚    df = pd.DataFrame({                                       â”‚
â”‚        'timestamp': time_windows,                            â”‚
â”‚        'node1_metric1': values1,                            â”‚
â”‚        'node1_metric2': values2,                            â”‚
â”‚        ...                                                   â”‚
â”‚    })                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. LAYER 6 - VALIDATION                                    â”‚
â”‚     Statistical Validation:                                  â”‚
â”‚       â”œâ”€> Check for NaN/Inf values                          â”‚
â”‚       â”œâ”€> Verify distribution moments match                 â”‚
â”‚       â””â”€> KS test for distribution fit                      â”‚
â”‚                                                              â”‚
â”‚     Logical Validation:                                      â”‚
â”‚       â”œâ”€> QoS boundary compliance                           â”‚
â”‚       â””â”€> Metric ordering (e.g., p50 < p95)                â”‚
â”‚                                                              â”‚
â”‚     Temporal Validation:                                     â”‚
â”‚       â”œâ”€> Autocorrelation structure                         â”‚
â”‚       â”œâ”€> Seasonality detection                             â”‚
â”‚       â””â”€> Change point detection                            â”‚
â”‚                                                              â”‚
â”‚     Quality Score = Î£ (weight_i * score_i)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 11. OUTPUT GENERATION                                        â”‚
â”‚     Formats:                                                 â”‚
â”‚       â€¢ CSV  â†’ df.to_csv()                                  â”‚
â”‚       â€¢ JSON â†’ df.to_json(orient='records')                 â”‚
â”‚       â€¢ Parquet â†’ df.to_parquet()                           â”‚
â”‚       â€¢ SAR Format â†’ export_sar_format() (custom)           â”‚
â”‚                                                              â”‚
â”‚     Metadata:                                                â”‚
â”‚       â€¢ Generation time                                      â”‚
â”‚       â€¢ Quality score                                        â”‚
â”‚       â€¢ Configuration summary                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    [COMPLETED]
```

---

## ğŸŒ **API ENDPOINTS**

#### **1. POST /api/generate**
- **AmaÃ§**: Sentetik data Ã¼retimi
- **Input**: JSON configuration (nodes, metrics, anomalies, etc.)
- **Process**: YukarÄ±daki flow'u Ã§alÄ±ÅŸtÄ±rÄ±r
- **Output**: Generated DataFrame + metadata

#### **2. GET /api/download**
- **AmaÃ§**: Ãœretilen veriyi indir
- **Params**: format (csv/json/parquet/sar)
- **Output**: File download

#### **3. GET /api/health**
- **AmaÃ§**: Servis saÄŸlÄ±k kontrolÃ¼
- **Output**: {"status": "healthy", "version": "2.1"}

---

## ğŸ”§ **TEMEL SINIFLAR VE DATA YAPILARI**

```python
# Core Classes
DataGenerator(config: GeneratorConfig)
  â””â”€ generate() â†’ pd.DataFrame

# Configuration Classes
GeneratorConfig      # Ana konfigÃ¼rasyon
NodeConfig          # Node tanÄ±mlarÄ±
MetricConfig        # Metrik tanÄ±mlarÄ±
DistributionConfig  # DaÄŸÄ±lÄ±m parametreleri
SeasonalityConfig   # Mevsimsellik ayarlarÄ±
ARIMAConfig         # ARIMA parametreleri
CorrelationConfig   # Korelasyon tanÄ±mlarÄ±
ChangePointConfig   # DeÄŸiÅŸim noktalarÄ±
AnomalyConfig       # Anomali senaryolarÄ±
ValidationConfig    # DoÄŸrulama kriterleri

# Enums
DistributionType    # normal, gamma, beta, etc.
AnomalyType        # spike, drop, congestion, etc.
ChangeType         # step, ramp
StabilityLevel     # high, medium, low
```

---

## ğŸ“ˆ **Ã–ZELLEÅME VE GENÄ°ÅLETME NOKTALARI**

1. **Yeni DaÄŸÄ±lÄ±m Ekleme**: `_generate_base_distribution()` metoduna yeni case
2. **Yeni Anomali Tipi**: `_apply_anomalies()` metoduna yeni elif bloÄŸu
3. **Custom Korelasyon**: `_apply_correlations()` metodunda Gaussian Copula yerine baÅŸka yÃ¶ntem
4. **ProsedÃ¼r Simulasyonu**: Layer 4'e state machine ekleyerek
5. **Yeni Validasyon**: `calculate_validation_score()` fonksiyonuna yeni metrik

---